{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[False False False False False False False False  True  True  True  True\n",
      " False False]\n",
      "[ 2  7  6  4 11 10  9  3  1  1  1  1  5  8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "np.random.seed(0);\n",
    "\n",
    "X = np.genfromtxt(\"data/X_train.txt\",delimiter = None)\n",
    "#X = X[:, [8,9,10,11]]\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\",delimiter = None)\n",
    "test_data = np.genfromtxt(\"data/X_test.txt\",delimiter = None)\n",
    "# feature extraction\n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "rfe = RFE(model, 4)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(fit.n_features_)\n",
    "print(fit.support_)\n",
    "print(fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc1 = StandardScaler()\n",
    "sc2 = StandardScaler()\n",
    "\n",
    "X = X[:, [0,8,9,10,11]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0, stratify=Y)\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "#fit_train = PCA(n_components = 4).fit(X_train)\n",
    "#X_train = fit_train.transform(X_train)\n",
    "sc1.fit(X_test)\n",
    "X_test = sc1.transform(X_test)\n",
    "#fit_test1 = PCA(n_components = 4).fit(X_test)\n",
    "#X_test = fit_test1.transform(X_test)\n",
    "sc2.fit(test_data)\n",
    "test_data = sc2.transform(test_data)\n",
    "#fit_test = PCA(n_components = 4).fit(test_data)\n",
    "#test_data = fit_test.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size=8, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(350, 350), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "rus = SMOTEENN(random_state=0)\n",
    "\n",
    "x_rus1,y_rus1 = rus.fit_sample(X_train,y_train)\n",
    "mlp1 = MLPClassifier(activation='logistic',solver='lbfgs',hidden_layer_sizes=(350,350),batch_size=8)\n",
    "mlp1.fit(x_rus1,y_rus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20076 12779]\n",
      " [ 6303 10842]]\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp1.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      0.61      0.68     32855\n",
      "        1.0       0.46      0.63      0.53     17145\n",
      "\n",
      "avg / total       0.66      0.62      0.63     50000\n",
      "\n",
      "0.621709750138\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))\n",
    "print(roc_auc_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = mlp1.predict_proba(test_data[:,[0,8,9,10,11]])\n",
    "# Create the data for submission by taking the P(Y=1) column from probs and just add a running index as the first column.\n",
    "Y_sub1 = np.vstack([np.arange(test_data.shape[0]), predictions1[:, 1]]).T\n",
    "\n",
    "# We specify the header (ID, Prob1) and also specify the comments as '' so the header won't be commented out with\n",
    "# the # sign.\n",
    "np.savetxt('data/Y_sub1.txt', Y_sub1, '%d, %.5f', header='ID,Prob1', comments='', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size=8, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(350, 350), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = y_train\n",
    "Y_train[y_train<1] = -1;\n",
    "x_rus2,y_rus2 = rus.fit_sample(X_train,Y_train)\n",
    "mlp2 = MLPClassifier(activation='tanh',solver='lbfgs',hidden_layer_sizes=(350,350),batch_size=8)\n",
    "mlp2.fit(x_rus2,y_rus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20217 12638]\n",
      " [ 6138 11007]]\n"
     ]
    }
   ],
   "source": [
    "Y_test = y_test\n",
    "Y_test[y_test<1] = -1;\n",
    "predictions = mlp2.predict(X_test)\n",
    "print(confusion_matrix(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.77      0.62      0.68     32855\n",
      "        1.0       0.47      0.64      0.54     17145\n",
      "\n",
      "avg / total       0.66      0.62      0.63     50000\n",
      "\n",
      "0.628667440767\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,predictions))\n",
    "print(roc_auc_score(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions2 = mlp2.predict_proba(test_data[:,[0,8,9,10,11]])\n",
    "# Create the data for submission by taking the P(Y=1) column from probs and just add a running index as the first column.\n",
    "Y_sub2 = np.vstack([np.arange(test_data.shape[0]), predictions2[:, 1]]).T\n",
    "\n",
    "# We specify the header (ID, Prob1) and also specify the comments as '' so the header won't be commented out with\n",
    "# the # sign.\n",
    "np.savetxt('data/Y_sub2.txt', Y_sub2, '%d, %.5f', header='ID,Prob1', comments='', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=8, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(350, 350), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp3 = MLPClassifier(activation='relu',solver='lbfgs',hidden_layer_sizes=(350,350),batch_size=8)\n",
    "mlp3.fit(x_rus1,y_rus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 20235 12620]\n",
      " [    0     0     0]\n",
      " [    0  6060 11085]]\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp3.predict(X_test)\n",
    "y_test[y_test<1] = 0\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.00      0.00      0.00     32855\n",
      "        0.0       0.00      0.00      0.00         0\n",
      "        1.0       0.47      0.65      0.54     17145\n",
      "\n",
      "avg / total       0.16      0.22      0.19     50000\n",
      "\n",
      "0.631216087336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Shashank\\AppData\\Local\\conda\\conda\\envs\\my_root\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))\n",
    "print(roc_auc_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions3 = mlp3.predict_proba(test_data[:,[0,8,9,10,11]])\n",
    "# Create the data for submission by taking the P(Y=1) column from probs and just add a running index as the first column.\n",
    "Y_sub3 = np.vstack([np.arange(test_data.shape[0]), predictions3[:, 1]]).T\n",
    "\n",
    "# We specify the header (ID, Prob1) and also specify the comments as '' so the header won't be commented out with\n",
    "# the # sign.\n",
    "np.savetxt('data/Y_sub3.txt', Y_sub3, '%d, %.5f', header='ID,Prob1', comments='', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ensembled version of these three models to be done tomorrow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
